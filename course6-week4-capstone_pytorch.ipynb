{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "<a href=\"http://cocl.us/pytorch_link_top\">\n    <img src=\"https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/DL0110EN/notebook_images%20/Pytochtop.png\" width=\"750\" alt=\"IBM Product \" />\n</a> "
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "<img src=\"https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/DL0110EN/notebook_images%20/cc-logo-square.png\" width=\"200\" alt=\"cognitiveclass.ai logo\" />"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "<h1><h1>Pre-trained-Models with PyTorch </h1>"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "In this lab, you will use pre-trained models to classify between the negative and positive samples; you will be provided with the dataset object. The particular pre-trained model will be resnet18; you will have three questions: \n<ul>\n<li>change the output layer</li>\n<li> train the model</li> \n<li>  identify  several  misclassified samples</li> \n </ul>\nYou will take several screenshots of your work and share your notebook. "
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "<h2>Table of Contents</h2>"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "<div class=\"alert alert-block alert-info\" style=\"margin-top: 20px\">\n\n\n<ul>\n    <li><a href=\"#download_data\"> Download Data</a></li>\n    <li><a href=\"#auxiliary\"> Imports and Auxiliary Functions </a></li>\n    <li><a href=\"#data_class\"> Dataset Class</a></li>\n    <li><a href=\"#Question_1\">Question 1</a></li>\n    <li><a href=\"#Question_2\">Question 2</a></li>\n    <li><a href=\"#Question_3\">Question 3</a></li>\n</ul>\n<p>Estimated Time Needed: <strong>120 min</strong></p>\n </div>\n<hr>"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "<h2 id=\"download_data\">Download Data</h2>"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "Download the dataset and unzip the files in your data directory, unlike the other labs, all the data will be deleted after you close  the lab, this may take some time:"
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "--2020-02-25 07:24:36--  https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/DL0321EN/data/images/Positive_tensors.zip\nResolving s3-api.us-geo.objectstorage.softlayer.net (s3-api.us-geo.objectstorage.softlayer.net)... 67.228.254.196\nConnecting to s3-api.us-geo.objectstorage.softlayer.net (s3-api.us-geo.objectstorage.softlayer.net)|67.228.254.196|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 2598656062 (2.4G) [application/zip]\nSaving to: \u2018Positive_tensors.zip\u2019\n\n100%[====================================>] 2,598,656,062 1.72MB/s   in 20m 15s\n\n2020-02-25 07:44:51 (2.04 MB/s) - \u2018Positive_tensors.zip\u2019 saved [2598656062/2598656062]\n\n"
                }
            ],
            "source": "!wget https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/DL0321EN/data/images/Positive_tensors.zip"
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {},
            "outputs": [],
            "source": "!unzip -q Positive_tensors.zip "
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "--2020-02-25 07:53:05--  https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/DL0321EN/data/images/Negative_tensors.zip\nResolving s3-api.us-geo.objectstorage.softlayer.net (s3-api.us-geo.objectstorage.softlayer.net)... 67.228.254.196\nConnecting to s3-api.us-geo.objectstorage.softlayer.net (s3-api.us-geo.objectstorage.softlayer.net)|67.228.254.196|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 2111408108 (2.0G) [application/zip]\nSaving to: \u2018Negative_tensors.zip\u2019\n\n100%[====================================>] 2,111,408,108 1.09MB/s   in 12m 20s\n\n2020-02-25 08:05:26 (2.72 MB/s) - \u2018Negative_tensors.zip\u2019 saved [2111408108/2111408108]\n\n"
                }
            ],
            "source": "! wget https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/DL0321EN/data/images/Negative_tensors.zip\n!unzip -q Negative_tensors.zip"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "We will install torchvision:"
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "Collecting torchvision\n\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7e/90/6141bf41f5655c78e24f40f710fdd4f8a8aff6c8b7c6f0328240f649bdbe/torchvision-0.5.0-cp36-cp36m-manylinux1_x86_64.whl (4.0MB)\n\u001b[K     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 4.0MB 15.6MB/s eta 0:00:01\n\u001b[?25hRequirement already satisfied: pillow>=4.1.1 in /opt/conda/envs/Python36/lib/python3.6/site-packages (from torchvision) (5.4.1)\nCollecting torch==1.4.0 (from torchvision)\n\u001b[?25l  Downloading https://files.pythonhosted.org/packages/24/19/4804aea17cd136f1705a5e98a00618cb8f6ccc375ad8bfa437408e09d058/torch-1.4.0-cp36-cp36m-manylinux1_x86_64.whl (753.4MB)\n\u001b[K     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 753.4MB 23kB/s s eta 0:00:01                       | 56.0MB 7.3MB/s eta 0:01:35     |\u2588\u2588\u2588\u2588\u2588\u258c                          | 129.5MB 17.1MB/s eta 0:00:37                 | 193.5MB 7.2MB/s eta 0:01:19 9.7MB/s eta 0:00:43     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a                 | 347.9MB 9.7MB/s eta 0:00:42MB/s eta 0:00:19  | 544.7MB 58.6MB/s eta 0:00:04\ufffd\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e       | 571.4MB 59.6MB/s eta 0:00:04\ufffd\ufffd\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588      | 613.7MB 50.2MB/s eta 0:00:03\ufffd\ufffd\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c     | 625.0MB 17.3MB/s eta 0:00:08\ufffd\ufffd\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c    | 647.3MB 49.7MB/s eta 0:00:03\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a    | 652.8MB 49.7MB/s eta 0:00:03\ufffd\ufffd\u2588\u2588\u2588\u2588    | 660.2MB 49.7MB/s eta 0:00:02\ufffd\ufffd\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f   | 663.7MB 49.7MB/s eta 0:00:02| 671.0MB 49.7MB/s eta 0:00:02\ufffd\ufffd\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b   | 673.2MB 49.7MB/s eta 0:00:02\ufffd\ufffd\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a   | 677.3MB 43.9MB/s eta 0:00:02\ufffd\ufffd\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589   | 679.1MB 43.9MB/s eta 0:00:02\ufffd\ufffd\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588   | 682.4MB 43.9MB/s eta 0:00:02\u2588\u2588\u2588\u2588   | 685.1MB 43.9MB/s eta 0:00:02\ufffd\ufffd\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f  | 687.9MB 43.9MB/s eta 0:00:02\ufffd\ufffd\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e  | 689.8MB 43.9MB/s eta 0:00:02\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d  | 692.6MB 43.9MB/s eta 0:00:02\ufffd\ufffd\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c  | 694.6MB 43.9MB/s eta 0:00:02\ufffd\ufffd\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b  | 696.1MB 43.9MB/s eta 0:00:02\ufffd\ufffd\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b  | 697.8MB 43.9MB/s eta 0:00:02 698.8MB 43.9MB/s eta 0:00:02     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a  | 700.4MB 43.9MB/s eta 0:00:02B 43.9MB/s eta 0:00:02\ufffd\ufffd\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589  | 703.2MB 61.0MB/s eta 0:00:01\ufffd\ufffd\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 748.8MB 53.6MB/s eta 0:00:01\n\u001b[?25hRequirement already satisfied: numpy in /opt/conda/envs/Python36/lib/python3.6/site-packages (from torchvision) (1.15.4)\nRequirement already satisfied: six in /opt/conda/envs/Python36/lib/python3.6/site-packages (from torchvision) (1.12.0)\nInstalling collected packages: torch, torchvision\nSuccessfully installed torch-1.4.0 torchvision-0.5.0\n"
                }
            ],
            "source": "!pip install torchvision"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "<h2 id=\"auxiliary\">Imports and Auxiliary Functions</h2>"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "The following are the libraries we are going to use for this lab. The <code>torch.manual_seed()</code> is for forcing the random function to give the same number every time we try to recompile it."
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": "<torch._C.Generator at 0x7fb3cbbc8f70>"
                    },
                    "execution_count": 5,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": "# These are the libraries will be used for this lab.\nimport torchvision.models as models\nfrom PIL import Image\nimport pandas\nfrom torchvision import transforms\nimport torch.nn as nn\nimport time\nimport torch \nimport matplotlib.pylab as plt\nimport numpy as np\nfrom torch.utils.data import Dataset, DataLoader\nimport h5py\nimport os\nimport glob\ntorch.manual_seed(0)"
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "metadata": {},
            "outputs": [],
            "source": "from matplotlib.pyplot import imshow\nimport matplotlib.pylab as plt\nfrom PIL import Image\nimport pandas as pd\nimport os"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "<!--Empty Space for separating topics-->"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "<h2 id=\"data_class\">Dataset Class</h2>"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": " This dataset class is essentially the same dataset you build in the previous section, but to speed things up, we are going to use tensors instead of jpeg images. Therefor for each iteration, you will skip the reshape step, conversion step to tensors and normalization step."
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "done\n"
                }
            ],
            "source": "# Create your own dataset object\n\nclass Dataset(Dataset):\n\n    # Constructor\n    def __init__(self,transform=None,train=True):\n        directory=\"/home/dsxuser/work\"\n        positive=\"Positive_tensors\"\n        negative='Negative_tensors'\n\n        positive_file_path=os.path.join(directory,positive)\n        negative_file_path=os.path.join(directory,negative)\n        positive_files=[os.path.join(positive_file_path,file) for file in os.listdir(positive_file_path) if file.endswith(\".pt\")]\n        negative_files=[os.path.join(negative_file_path,file) for file in os.listdir(negative_file_path) if file.endswith(\".pt\")]\n        number_of_samples=len(positive_files)+len(negative_files)\n        self.all_files=[None]*number_of_samples\n        self.all_files[::2]=positive_files\n        self.all_files[1::2]=negative_files \n        # The transform is goint to be used on image\n        self.transform = transform\n        #torch.LongTensor\n        self.Y=torch.zeros([number_of_samples]).type(torch.LongTensor)\n        self.Y[::2]=1\n        self.Y[1::2]=0\n        \n        if train:\n            self.all_files=self.all_files[0:30000]\n            self.Y=self.Y[0:30000]\n            self.len=len(self.all_files)\n        else:\n            self.all_files=self.all_files[30000:]\n            self.Y=self.Y[30000:]\n            self.len=len(self.all_files)     \n       \n    # Get the length\n    def __len__(self):\n        return self.len\n    \n    # Getter\n    def __getitem__(self, idx):\n               \n        image=torch.load(self.all_files[idx])\n        y=self.Y[idx]\n                  \n        # If there is any transform method, apply it onto the image\n        if self.transform:\n            image = self.transform(image)\n\n        return image, y\n    \nprint(\"done\")"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "We create two dataset objects, one for the training data and one for the validation data."
        },
        {
            "cell_type": "code",
            "execution_count": 8,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "done\n"
                }
            ],
            "source": "train_dataset = Dataset(train=True)\nvalidation_dataset = Dataset(train=False)\nprint(\"done\")"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "<h2 id=\"Question_1\">Question 1</h2>"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "<b>Prepare a pre-trained resnet18 model :</b>"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "<b>Step 1</b>: Load the pre-trained model <code>resnet18</code> Set the parameter <code>pretrained</code> to true:"
        },
        {
            "cell_type": "code",
            "execution_count": 9,
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": "Downloading: \"https://download.pytorch.org/models/resnet18-5c106cde.pth\" to /home/dsxuser/.cache/torch/checkpoints/resnet18-5c106cde.pth\n"
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "9a0807a0aa1e48939ad5fa5eff7a2c99",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": "HBox(children=(IntProgress(value=0, max=46827520), HTML(value='')))"
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "\n"
                }
            ],
            "source": "# Step 1: Load the pre-trained model resnet18\nmodel = models.resnet18(pretrained=True)"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "<b>Step 2</b>: Set the attribute <code>requires_grad</code> to <code>False</code>. As a result, the parameters will not be affected by training."
        },
        {
            "cell_type": "code",
            "execution_count": 10,
            "metadata": {},
            "outputs": [],
            "source": "# Step 2: Set the parameter cannot be trained for the pre-trained model\nfor param in model.parameters():\n    param.requires_grad = False"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "<code>resnet18</code> is used to classify 1000 different objects; as a result, the last layer has 1000 outputs.  The 512 inputs come from the fact that the previously hidden layer has 512 outputs. "
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "<b>Step 3</b>: Replace the output layer <code>model.fc</code> of the neural network with a <code>nn.Linear</code> object, to classify 2 different classes. For the parameters <code>in_features </code> remember the last hidden layer has 512 neurons."
        },
        {
            "cell_type": "code",
            "execution_count": 11,
            "metadata": {},
            "outputs": [],
            "source": "model.fc = nn.Linear(512, 2)"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "Print out the model in order to show whether you get the correct answer.<br> <b>(Your peer reviewer is going to mark based on what you print here.)</b>"
        },
        {
            "cell_type": "code",
            "execution_count": 12,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "ResNet(\n  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (relu): ReLU(inplace=True)\n  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n  (layer1): Sequential(\n    (0): BasicBlock(\n      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (1): BasicBlock(\n      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n  )\n  (layer2): Sequential(\n    (0): BasicBlock(\n      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (downsample): Sequential(\n        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (1): BasicBlock(\n      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n  )\n  (layer3): Sequential(\n    (0): BasicBlock(\n      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (downsample): Sequential(\n        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (1): BasicBlock(\n      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n  )\n  (layer4): Sequential(\n    (0): BasicBlock(\n      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (downsample): Sequential(\n        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (1): BasicBlock(\n      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n  )\n  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n  (fc): Linear(in_features=512, out_features=2, bias=True)\n)\n"
                }
            ],
            "source": "print(model)"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "<h2 id=\"Question_2\">Question 2: Train the Model</h2>"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "In this question you will train your, model:"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "<b>Step 1</b>: Create a cross entropy criterion function "
        },
        {
            "cell_type": "code",
            "execution_count": 17,
            "metadata": {},
            "outputs": [],
            "source": "# Step 1: Create the loss function\ncriterion = nn.CrossEntropyLoss()"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "<b>Step 2</b>: Create a training loader and validation loader object, the batch size should have 100 samples each."
        },
        {
            "cell_type": "code",
            "execution_count": 13,
            "metadata": {},
            "outputs": [],
            "source": "train_loader = torch.utils.data.DataLoader(\n    dataset=train_dataset,\n    batch_size=100)\n\nvalidation_loader = torch.utils.data.DataLoader(\n    dataset=validation_dataset,\n    batch_size=100)"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "<b>Step 3</b>: Use the following optimizer to minimize the loss "
        },
        {
            "cell_type": "code",
            "execution_count": 15,
            "metadata": {},
            "outputs": [],
            "source": "plist = [p for p in model.parameters() if p.requires_grad]\n\noptimizer = torch.optim.Adam(plist, lr=0.001)"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "<!--Empty Space for separating topics-->"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "**Complete the following code to calculate  the accuracy on the validation data for one epoch; this should take about 45 minutes. Make sure you calculate the accuracy on the validation data.**"
        },
        {
            "cell_type": "code",
            "execution_count": 18,
            "metadata": {},
            "outputs": [],
            "source": "n_epochs = 1\nloss_list = []\naccuracy_list = []\naccuracy = 0\ncorrect = 0\nN_test = len(validation_dataset)\nN_train = len(train_dataset)\nstart_time = time.time()\n#n_epochs\n\nrunning_loss = 0\nstart_time = time.time()\nfor epoch in range(n_epochs):\n    for x, y in train_loader:\n        model.train() # train model\n        optimizer.zero_grad() # clear gradient \n        z = model(x) #  make a prediction \n        loss = criterion(z, y) # calculate loss \n        loss.backward() # calculate gradients of parameters \n        optimizer.step() # update parameters \n        loss_list.append(loss.data)\n        \n    correct = 0\n    for x_test, y_test in validation_loader:\n        model.eval() # evaluate model\n        z = model(x_test) # make a prediction \n        _, yhat = torch.max(z.data, 1) # find max \n        correct += (yhat==y_test).sum().item() # count misclassified samples\n    \n    accuracy=correct/N_test"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "<b>Print out the Accuracy and plot the loss stored in the list <code>loss_list</code> for every iteration and take a screen shot.</b>"
        },
        {
            "cell_type": "code",
            "execution_count": 19,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": "0.9935"
                    },
                    "execution_count": 19,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": "accuracy"
        },
        {
            "cell_type": "code",
            "execution_count": 20,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEKCAYAAAD9xUlFAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvOIA7rQAAIABJREFUeJzt3XecVPW9//HXZ2dne690ll4jxQXBXhFNojEmljS9iTFGjSVVf8n1eo1p5kZj7jVRkhhNYsSuJBKxIaDUpbPUZVnYZXvvdb6/P86Zs7MNFthhdpnP8/HgwcyZszPfswPznm8XYwxKKaUUQEigC6CUUmrw0FBQSinl0FBQSinl0FBQSinl0FBQSinl0FBQSinl0FBQSinl0FBQSinl0FBQSinlCA10AU5USkqKycjICHQxlFJqSNm8eXO5MSb1eOcNuVDIyMggKysr0MVQSqkhRUQO9+c8bT5SSinl0FBQSinl0FBQSinl0FBQSinl0FBQSinl0FBQSinl0FBQSinlCJpQ2JRXya9X7MXj0e1HlVKqL0ETCtvzq3lq5UEaWtsDXRSllBq0giYUYsKtydv1LRoKSinVl6AJhWhvKDRrKCilVF+CJhRiIrSmoJRSxxM0oRCrzUdKKXVcQRMKTk1Bm4+UUqpPfg0FEVksIvtEJEdEHujl8SdEZJv9Z7+IVPurLNFhVijUaU1BKaX65Lf9FETEBTwFXAEUAJtEZJkxZrf3HGPM/T7nfweY46/yxGpNQSmljsufNYX5QI4xJtcY0wosBa49xvk3Ay/6qzDe0UcNWlNQSqk++TMURgL5PvcL7GM9iMhYYBzwYR+P3y4iWSKSVVZWdlKFcbtCiHCHaEezUkodgz9DQXo51tcaEzcBrxpjOnp70BizxBiTaYzJTE097hajfYoJd2ufglJKHYM/Q6EAGO1zfxRQ2Me5N+HHpiOvmHAXL2/KZ9p/vkNTa6/5o5RSQc2fobAJmCQi40QkDOuDf1n3k0RkCpAIrPNjWQBrWGq7x9DU1kFZXYu/X04ppYYcv4WCMaYduBtYAewBXjbGZIvIIyJyjc+pNwNLjTF+X77Uu/4R6CQ2pZTqjd+GpAIYY5YDy7sde6jb/Yf9WQZfMeFu53Zdc9vpelmllBoygmZGM1h9Cl61Ol9BKaV6CKpQaO3wOLe1pqCUUj0FVSiU17c6t2ubNBSUUqq7oAqFivrOEUd12nyklFI9BFUo/PRzM5k1Kh5XiFCrzUdKKdVDUIXCuRNSeOvu80mODtOaglJK9SKoQsErNiJUawpKKdWLoAyFuEi31hSUUqoXQRkKsRFuHX2klFK9CNJQCNWaglJK9SIoQyEuwq0zmpVSqhdBGgra0ayUUr0JzlCIdNPa7qG5TfdUUEopX0EZCrER1uKw2q+glFJdBWUoRIVZoaC7rymlVFdBGQqRbmsJ7SZtPlJKqS6CMxTCrMvWUFBKqa6CMhQivDUFbT5SSqku/BoKIrJYRPaJSI6IPNDHOTeIyG4RyRaRf/izPF5On0KbdjQrpZQvv+3RLCIu4CngCqAA2CQiy4wxu33OmQQ8CJxnjKkSkTR/lceX06fQ6jnOmUopFVz8WVOYD+QYY3KNMa3AUuDabud8E3jKGFMFYIwp9WN5HNrRrJRSvfNnKIwE8n3uF9jHfE0GJovIJyKyXkQW+7E8jsgwb01Bm4+UUsqX35qPAOnlmOnl9ScBFwOjgDUiMtMYU93liURuB24HGDNmzCkXzAkFrSkopVQX/qwpFACjfe6PAgp7OectY0ybMeYQsA8rJLowxiwxxmQaYzJTU1NPuWDap6CUUr3zZyhsAiaJyDgRCQNuApZ1O+dN4BIAEUnBak7K9WOZAHCFCGGhITTq6COllOrCb6FgjGkH7gZWAHuAl40x2SLyiIhcY5+2AqgQkd3ASuAHxpgKf5XJV6TbRbPOU1BKqS782aeAMWY5sLzbsYd8bhvgu/af0yrS7dI+BaWU6iYoZzQDRIW5aGrTPgWllPIVtKEQ4XbpkFSllOomaEMhMkybj5RSqrugDYWoMJcuiKeUUt0EbShEuF00aigopVQXQRsKkW6X7tGslFLdBG0oRGmfglJK9RC0oaDNR0op1VPQhkJkmDYfKaVUd0EbClFuF20dhrYOncCmlFJeQRsK3uWztbaglFKdgjYUIpzlszUUlFLKK2hDIT0uAoCC6qYAl0QppQaPoA2FyekxABwoqQtwSZRSavAI2lAYnRhFhDuEfcX1gS6KUkoNGkEbCiEhwuT0WPaX1FFUo01ISikFQRwKAJPTY/k4p5yFv/iQQu1bUEqp4A6FMUlRzu3DFY0BLIlSSg0Ofg0FEVksIvtEJEdEHujl8VtFpExEttl/bvNnebq7/uxRXDwlFYDSuubT+dJKKTUo+S0URMQFPAVcBUwHbhaR6b2c+pIxZrb950/+Kk9vRiZE8uRNcwAoq2s5nS+tlFKDkj9rCvOBHGNMrjGmFVgKXOvH1zspcRGhhIeGUKqhoJRSfg2FkUC+z/0C+1h314vIDhF5VURG+7E8vRIR0uLCKa3V5iOllPJnKEgvx0y3+/8EMowxZwHvA8/3+kQit4tIlohklZWVDXAxIS02gpJarSkopZQ/Q6EA8P3mPwoo9D3BGFNhjPF+Gv8ROLu3JzLGLDHGZBpjMlNTUwe8oGmx4drRrJRS+DcUNgGTRGSciIQBNwHLfE8QkeE+d68B9vixPH2yQkFrCkopFeqvJzbGtIvI3cAKwAU8a4zJFpFHgCxjzDLgHhG5BmgHKoFb/VWeY0mLi6CuuZ3mtg5n9VSllApGfgsFAGPMcmB5t2MP+dx+EHjQn2Xoj7TYcABKa1sYkxx1nLOVUurMFdQzmr3S7GW0tV9BKRXsNBTwqSlov4JSKshpKNAZCiU6V0EpFeQ0FIDEqDBCQ0RrCkqpoKehgLW3QmpsOKU6gU0pFeQ0FGw6gU0ppTQUHGlxEbpSqlIq6Gko2HRWs1JKaSg40mIjqGxopbXdE+iiKKVUwGgo2NLirGGpZfVaW1BKBS8NBduIhEgAjuhezUqpIKahYJs+PA6A7MKaAJdEKaUCR0PBlhobTlpsOLsLawNdFKWUChgNBR8zR8azS2sKSqkgpqHgY+aIOHJK62lq7Qh0UZRSKiA0FHxMGx6Hx8DBsvpAF0UppQJCQ8GHd1hquQ5LVUoFKQ0FHwlRYQBUN7YFuCRKKRUYGgo+kuxQqGpsDXBJlFIqMPwaCiKyWET2iUiOiDxwjPO+ICJGRDL9WZ7jiYt0IwJVWlNQSgUpv4WCiLiAp4CrgOnAzSIyvZfzYoF7gA3+Kkt/uUKE+Eg3VQ1aU1BKBSd/1hTmAznGmFxjTCuwFLi2l/N+CjwGDIrNDBKjwrT5SCkVtPwZCiOBfJ/7BfYxh4jMAUYbY/7lx3KckMQot3Y0K6WCVr9CQUTuFZE4sfxZRLaIyKLj/Vgvx4zPc4YATwDf68fr3y4iWSKSVVZW1p8inzStKSilgll/awpfN8bUAouAVOA/gF8e52cKgNE+90cBhT73Y4GZwEcikgcsAJb11tlsjFlijMk0xmSmpqb2s8gnJyEqTPsUlFJBq7+h4P3WfzXwF2PMdnqvCfjaBEwSkXEiEgbcBCzzPmiMqTHGpBhjMowxGcB64BpjTNYJXcEAS4xy6+gjpVTQ6m8obBaRd7FCYYU9YuiYW5QZY9qBu4EVwB7gZWNMtog8IiLXnEqh/SkxOoymtg6a23T9I6VU8Ant53nfAGYDucaYRhFJwmpCOiZjzHJgebdjD/Vx7sX9LItfJfrMah4W7wpwaZRS6vTqb01hIbDPGFMtIl8BfgKckWtMJ0a5AajUfgWlVBDqbyj8AWgUkVnAD4HDwF/9VqoAGhYfAcDR6qYAl0QppU6//oZCuzHGYE0+e9IY8yTW6KEzzsS0GAD2l9QFuCRKKXX69bdPoU5EHgS+ClxgL2Hh9l+xAic2ws3IhEj2FWsoKKWCT39rCjcCLVjzFYqxZib/2m+lCrDJ6TFaU1BKBaV+hYIdBC8A8SLyGaDZGHNG9ikATE6PZW9xHSv3lmK1mimlVHDo7zIXNwAbgS8CNwAbROQL/ixYIE1Ot7pL/uO5TXySUxHg0iil1OnT3+ajHwPzjDG3GGO+hrUC6n/6r1iBddGUVOaOSQB0v2alVHDpbyiEGGNKfe5XnMDPDjkpMeG89u1ziQpzkVfREOjiKKXUadPf0UfviMgK4EX7/o10m6l8phERxiZHc7iiMdBFUUqp06ZfoWCM+YGIXA+ch7UQ3hJjzBt+LdkgkJEcpaOQlFJBpb81BYwxrwGv+bEsg86Y5Cg+2FNKh8fgCjneorBKKTX0HTMURKQOn41xfB8CjDEmzi+lGiQykqNp7fBQVNPEqMSoQBdHKaX87pihYIw5I5ey6K+xyVYQHCpv0FBQSgWFM3YE0UDwzlfQJS+UUsFCQ+EYUmLCSYsNZ3dRbaCLopRSp4WGwnFMGx7HniKtKSilgoOGwnFMGx5HTmkdre3H3H1UKaXOCH4NBRFZLCL7RCRHRB7o5fE7RGSniGwTkY9FZLo/y3Mypg2Ppa3D6HIXSqmg4LdQsPdceAq4CpgO3NzLh/4/jDGfMsbMBh4DHvdXeU7WhFRr0x2d2ayUCgb+rCnMB3KMMbnGmFZgKdbObQ5jjG8PbjS9z4kIqPhIay+h2qa2AJdEKaX8r98zmk/CSCDf534BcE73k0TkLuC7QBhwqR/Lc1LivKHQrKGglDrz+bOm0Nu6ED1qAsaYp4wxE4AfAT/p9YlEbheRLBHJKisrG+BiHltseCgiUKM1BaVUEPBnKBQAo33ujwIKj3H+UuBzvT1gjFlijMk0xmSmpqYOYBGPLyREiA0P1eYjpVRQ8GcobAImicg4EQkDbgKW+Z4gIpN87n4aOODH8py0+Ci31hSUUkHBb30Kxph2EbkbWAG4gGeNMdki8giQZYxZBtwtIpcDbUAVcIu/ynMq4iLc1Da3B7oYSinld/7saMYYs5xum/EYYx7yuX2vP19/oMRHak1BKRUcdEZzP8RFuLVPQSkVFDQU+kFrCkqpYKGh0A9xkaHUNrfpXAWl1BlPQ6Ef4iPdNLd5OOvhd/loX2mgi6OUUn6jodAP3lnNACv3aigopc5cGgr9EO8TCutzKwNYEqWU8i8NhX6Ii+gMhX0ldVQ2tAawNEop5T8aCv1hr+KUFB0GQFae1haUUmcmDYV+WDg+ma8uGMsbd54LQI5uuKOUOkP5dUbzmSLC7eKnn5sJQFpsOIfKGgJcIqWU8g+tKZygjJRoDpVrKCilzkwaCidofEo0eRUaCkqpM5OGwgnKSImmvL5Vl71QSp2RNBRO0LiUaAAyH32Pq59cw/b86gCXSCmlBo6GwgmalBYDQFuHYX9JHe9kFwNQ39KOMT12G1VKqSFFQ+EEjU+N4cVvLmD7fy1icnosu47WUNvcxtk/fY8v/XEDbR2eQBdRKaVOmobCSVg4IZn4SDczR8aRXVjL5sNVtLR7WJdbwdJN+YEunlJKnTQNhVMwc2Q8lQ2t/HN7Ia4Qa9pzQVVjgEullFInT0PhFMwcGQ/A61uOMnNEHKmx4bpDm1JqSPNrKIjIYhHZJyI5IvJAL49/V0R2i8gOEflARMb6szwDbfrwOEYnRQIwZ0wi8ZFuqhs1FJRSQ5ffQkFEXMBTwFXAdOBmEZne7bStQKYx5izgVeAxf5XHHyLcLt6663y+ecE4bj03Q7ftVEoNef6sKcwHcowxucaYVmApcK3vCcaYlcYYbyP8emCUH8vjF0nRYfz409PJSInWUFBKDXn+DIWRgO9QnAL7WF++Afy7twdE5HYRyRKRrLKysgEs4sDSUFBKDXX+DAXp5Vivs7tE5CtAJvDr3h43xiwxxmQaYzJTU1MHsIgDyzcU/vDRQXYU6GxnpdTQ4s9QKABG+9wfBRR2P0lELgd+DFxjjGnxY3n8Lj7STV1zOw0t7fzqnb08+/GhQBdJKaVOiD9DYRMwSUTGiUgYcBOwzPcEEZkDPIMVCKV+LMtp4d3LeW9xHQBbdV0kpdQQ47dQMMa0A3cDK4A9wMvGmGwReURErrFP+zUQA7wiIttEZFkfTzckeENhd1EtAIcrGqmoH9KVH6VUkPHrzmvGmOXA8m7HHvK5fbk/X/9084bCHjsUALblV3PZtPRAFUkppU6IzmgeQPFRdk2hsJYQAVeIsOVIVZdz3tlVrB3QSqlBS0NhAPnWFIbFRTBteCxbj1RT29xGW4cHYww/em0Hv195MMAlVUqp3mkoDCBvKLS0exiREMncMYlsz69m0eOrefy9/c6ObYU1TQEuqVJK9U5DYQB5QwEgPS6COWMSaGjtoLi2mc15VRwotUYlFVZrKCilBicNhQEU4XZx9yUTAWvbzrljEp3H9hbXcrC0HoDy+laa2zoCUkallDoWv44+Ckbfv3IKi2cOY1xKNFFhLiamxdDe4SGvopE1B8qd84pqmp39npVSarDQmoIfzBwZT3R4KCLCe/dfyGNfmAXAu7tLCLU34zlapU1ISqnBR0PBz0SEKcNinfsLJyQD2q+glBqcNBROg/hIN2ePTWT+uCR+88VZiMBRDQWl1CCkfQqnyat3LETEajpKjQknX/dyVkoNQlpTOE28gQBw9thEVu8v5987i9iQWxHAUimlVFcaCgHwmbNGUF7fwrdf2MIDr+/EmF63mVBKqdNOQyEALp2aRlSYC4BD5Q1sOVLN7sJadhfW9vkzh8obWHuwvM/HlVJqIGgoBEBkmItfXX8WT3/lbCLdLt7YWsDVv1vD1b9bQ2u7xzkvt6yebfaeDA+9tYt7XtwaqCIrpYKEhkKAfHbWCBbPHMas0fHsLapzjv9ze+fmdD9fvpc7/76Z6sZW1h2s0JnQSim/01AIsLTYCErrWggLtd6Kn769mwdf38nf1x/mSGUDhTXNvLq5gHaP1e9QWttCXXMbh8obAPjb+sPMeOgdPB7tl1BKnTodkhpgabHhFFQ14jHwlQVj2JBbyYsbj5AUHebUCp5e1bnUdlFNE39dl8erWwrY8pMr+Ok/d9Pa4aG0roVh8REBugql1JlCawoBlhYXjvdLfubYJN69/0LuvWwSlQ2tNLZaoVBe38oFk1IAKK5tZm9xHdWNbRTVNpNgb+xzuKIhIOVXSp1Z/BoKIrJYRPaJSI6IPNDL4xeKyBYRaReRL/izLINVelznt/vU2HBEhGnDY3ucd+u5GYC1kF5umbXaak5pPYlRYYC1H7SvhpZ23ttd4qdSK6XOVH4LBRFxAU8BVwHTgZtFZHq3044AtwL/8Fc5BrvU2HDndkqMdXtiWmcojE2OIjk6jIsmpxIbEUpuWT2FNc0AHCytJybCagE8XNm1pvBKVj7f/GsW+ZU6c1op1X/+7FOYD+QYY3IBRGQpcC2w23uCMSbPfszT2xMEg7TYrjUFsIIgNERo9xj+9LVMOowh1BXC8PgI1udWOufnlNXT0NIOQF63moK3Izq/spHRSVH+vgyl1BnCn81HI4F8n/sF9jHlIy3OCoLQECHB3rnN7QohIyWahCg3k9JjmTosDoBh8ZEcsb/5x0WEklNaT21TGwBHuoWCNyQK+rnwXlldy6lfjFJqyPNnKEgvx05q3KSI3C4iWSKSVVZWdorFGlxiw0OJcIeQHBNGSEjnr2z+uCRmjUrocu645M5v/JdMTeNgaT01dijkVTR0WS7D2/FcUNXEwbJ6vv7cJmqb23otw4GSOub//H3WHey5DtPanHIaW9tP/gKVUkOKP0OhABjtc38UUNjHucdkjFlijMk0xmSmpqYOSOEGCxEhLTaiS98CwKPXzuQvt87rcuzeyydzzawRLJ4xjCnDYqloaKWhtYMR8RHUNbfz9s4iHl6WTWldMwX2Jj4FVY18uKeUD/eW8tE+K1B3F9bypT+ud2oH2wtqMAa2HKnq8nr7S+r40p828PPlewb0mpvbOjRolBqk/NmnsAmYJCLjgKPATcCX/Ph6Q1ZmRiIx4V3fCt9ag1dSdBi/u3kOAG9tO+oc/+zsETyzKpf7X9pGW4fhubV5zmMFVU24Q6zsX7WvjEunpnH179YAsPZgOdfOHsmBUmtG9Z6irmsv7S22jg/0LnE/em0H1Y1tPP/1+QP6vEqpU+e3moIxph24G1gB7AFeNsZki8gjInINgIjME5EC4IvAMyKS7a/yDGaP3zCbR66deUI/MyIh0rk9JT2WacPjaOswXD4tvfOc+AiOVjWRYw9hXbW/jA/3ljqP77M/9A+UWI97Q8DLGxJJ0V1rMafqYFk9+0vqjn/icbR3eMgpPfXnUUp18us8BWPMcmPMZGPMBGPMz+xjDxljltm3NxljRhljoo0xycaYGf4sz5nENxTiI90smp6O2yX897UzmDPG6ou4YFIqRTVN7CuuIz7STXl9C+/sKkIEMpKjnBDw1hRyy+q7rK2062gNgNNvMVAq6lsprWuhvePUBp29ua2QRU+s7tHJPpRV1LewfGfRCf3M4YoGZjz0DgcGIGiV0hnNQ1R6bDguu4kpPtLNty+ewIr7LmRkQiQvfnMBr9yxkHPGJ+ExUN/SzmdnDQfg/T2ljEqMZPboBD7cW8rFv15JfmUT04bH4TGwKa+SCx77kNv/msV2e4XW8vrOkUkej+Guf2xhzYHODv+fL9/Dj17dQUE/dpMzxlBR30qHx1Be33pKv4MDJXV4DKzJCdzggz1FtQMaSkvW5HLnC1uo62NQQG/2l9TT0NpBTmn9gJVDBS8NhSEq1BXCMHs2dFykmwi3i/GpMQBEuF3My0ji6k8Nd86/fFo6UWEuWts9TEiNYYo9zNU7dPW6OSMAWLopn/zKJtYcKKe22eoM9g2FnLJ63t5RxPKdxYAVOEtW5/JSVj4/X74HYwzLdxY5TVPd1bW002rXEIpqOvsqnlqZw23Pb+py7s6CGv60Jte5//aOIjIffc+pzXi3NF2bc+zd60rrmrni8VXsLe57vwovYwxPvLe/X+cC3P/SNh59e/fxT+ynLYetzv6qhv6HQoX9/vQ1uuxE7S6spbSueUCeSw09GgpD2IgEKxTi7fkN3UW4Xfz2xtnER7o5a1QC04dbQTAhNcZZSuOeSyey5Ktn8/XzxhHpdvHBHmtpjFU/uJgXv7mAW8/NoLy+xRnuuinPmjznXWpjn/3hGRsRysZDVfzw1R3c+cIWvv/K9h7l+fnyPfzA53hJbecHz0ub8vlgb2mXb8if/b+PefTtPU4I/HrFXsrrW53NhrxzNtYeLD/mKrFr9pdzoLSeX/57b5/neNU2t/PkBwd4Y2tnR/5rmwv67AMprWvhaD/nghxPa7uH7QVWk11VY/9rURUN1rm1Tac+oiuntI6rf7eG773c8/1TwUFDYQjz9iv0FQoAn5szkm0PXUFSdBgzR8YDVihcNDmVN+48l/uvmMyiGcMIdYUweVgszW0eUmLCSYuLYOGEZIbHR9Dc5uEX/97L5sNVbM6zvsnm2jOmvbvFecPjlc0FAOwtrqWptYN7l251Poz/ub2QFdmd6zEV2ct15JU3cKSyEWNgp92P4Z2pDTgfusPjrev1rumUX9lEbHgoVY1tHK5spKimifuWbu0x3LXBvt+f5pVK+wO2qNoqW0t7Bz94dTt/XnOox7kdHkNVY2uXcDsV2YU1ziZLlScQCuUDWFN44LWdAAMWdGro0VAYwqYOiyM1Npzw0GO/jSJW34M3FCamxSAizBmT6DwGMG2YVXuYnB7jHPOux7RkdS7X/2Etr9vfoMvqWvj9Rzm8nFVAfKSba2dbzU9ul/D4DbNo6zCs3FfK2zuKeG93MbXNbU4IeBXb91ft7+wT2J5vhYJvn4W3RuBtbnpvdynVja3UNLVxzvgkwJqP8f7uEt7cVsgO+9u2t3bjHVJ7tLqJmsauH5wdHsPj7+5z1ojyNsV4X+tQeQMe07m21Ns7ith1tIaPD5Sz4VAFxlir2LadYqc5wObDnfNEqn1CIae0nu+/sr3PDZYq6r01hVMLhfzKRrLsMgzE9SjrS9O9S7cOqd+nhsIQdtsF43j//ou6fLAfy2fOGs4vPv8pMscm9vr4VCcUOhfkS44Jc27Pz0giNjyUq2YOA+Cxd/ax82gNKTFhTEiNYWRCJJ85awRXzRxOmCuEJ97bT7vHcKi8gR32h71XVJiLoppmjDG8ue0o41KiyUiOcjq3Vx/o3I/6pY35fPelbeRXNZEQZY2i2nDIasZaOMFaUtyaud05i3tPUS0z/2sFu47WON96jYH1hzr7H6obW9mUV8nvPszh9S1W2Hk7vwvtmoJ3uG5euRUaD7y+g6dW5nDfS1t56K3OEdSl/VwmpK65rUtfiq8tR6qcWp9vn8Lj7+3j1c0FTtNddxUN3prCqTUfrdxnDVe+auYwimua6dCNm07ZR/tLeWtbYZ99bIORhsIQ5naFEB/Vd9NRdxFuFzfPH9PrxDiAaXafw8S0njUFgJfvWMjO/76S7y2a3OXnLpyciojw1t3n8fPrPkVkmIvFM4dxwG6u8Ziu24yCFUDZhTW8sfUoW49Uc9sF45g1OoGsw1V4PIbswloWjE8iPDSEd7KLeX3rUTo8hosnWzPaV9u1i3kZibhCxJqPYb9efmUjaw9W0NDawd/WHaawuonZoxMQ6WzuyimtY97P3ucHr1pt5/tKasktq6fY/sAuqbU+FL3XUFzbTGltM3XN7WzKq6K8vpWDZZ3NUcU1PZuQmts6aGrt+u3+F//eyw3PrOtxrjGGzYeruHByKiHS2adwpKKRd3ZZnfobD1XS3NbBa5sLuixp4q0pnOrQ4Q/2lDI+JZrzJqbQ1mHOuPWwNh6qPKmmvjUHyli2/aQWY6C8znpvus8BGsw0FJTj7LGJfPeKyXzmrM5RS94F+86bmOwcG5MUDcDMkXGsf/AyfrR4KmAFSGSYC4CffHoasRGhjE+1zn1z21GnmSs+0s0t52aQV9HId1/ezuikSL549mgumZJGeX0LWYer2FtUy4wR8YzptsLrxVPSAJxJeONSohkeH0FBVaPzIV1Q1UR2oVUz+eeOQvaX1DMxLYZxydHOhLwlq3Np6zDkV1ohsCG3kit/u5qXJkNiAAAYiUlEQVSnPrJ2uWv3GMrrW7pMjluXa9UyvG34Pp/LlPbyYfOj13Zwy182djm25XAV+ZVNPfo9jlY3UVLbwryMROIj3U4ovLqlAIO1cu6G3Ere2naU772ynW12jcoqT8/mI2MM/9xe2O/lRDwew8ZDlVwwKYWRdl/VmdSv0OEx3PLsRn71zvEHG3T3zKpcfnmSS714a3H7+jmabTDQUFCOUFcI91w2iYSoziajtNgI/nLrPJZ8NdM5FhYawor7LuTlby1kWHwEEW5Xj+dKi4vgjTvP5e/fOAeAlnYPU4fHMTw+guSYMK6dPZK37jqPJ26cxZt3nkdYaAiXTkvD7RL+8FEOLe0eZoyI6xEK59i1h6KaZsanRhMb4WZUYiR7i+ucPouCqkZ2F9YyMiGSxtYO6lvaGZEQybThcewprmXrkSre3FrIeROTcbuEcSnRVDS09vh2XFjdxP6SetLtYPwkp5y+FNc2862/ZfFDu+ZhjGHtwQq2Hqni8Xf3cfc/ttDS3jmXoKCqiZb2DidgvP0Jc8ckkhgVRpXd9/FudjHzxiZx5YxhbMuv5mN7+G2eveChx2OobOjZ0ZxdWMt3XtzKW9sK2ZZf3Wd/hG/5m9o6mJQe6wxgKBzgUHhnVzE3PL0uIPuJH65ooKmtg3UHK7rUsvqjtK6Zwprm4/4Oe+OtxZ1qTcHjMTz2zl5nQqk/aSio47pkahrR3dZmmjIslqiwYy+dNTHN+oC5bs5IJqfH8J+fnsacMQlMtOdTzBwZz3VzRpFsN1HFRbg5f2IKK+2F+6aPiHP2gnj8hlksnjGM9NgIMpKt2od3FdlRiZ2zs2PDQ8ktbyCntJ5rZ49ghL1vdXpcONOGx5Jf2cR1v1/L8IQIfvPF2ax/8DJ+eOWULuX2dtF84el15JTW87k51orva3tZRRasZc9f2pTPiuwSXttylNK6Zoprmymra6Gtw/D0qlz+taOID/eU0m5/IB6paOTx9/Zz+eOrnF3y4iPdTB0WS2J0GNWNrRypaGRvcR2LZqRz/sQUWjs8vL3DasY4XGGNtrrhmXXOdq41TW28te0oDy/LZrXdUb/2YAWfe+oTLv2fj45Za/DuvzE+JdoZ6jzQobDmQBkb8ypPaw3kf1bsY+uRKvbbfUNFNc3OLoXe2pS3VtkXb39R990NuzPG9Agcb+jvKeo9FIwxZOVVHjeo9pfW8fuPDp6WZigNBeV3T9w4m3fvv4jMjCSeuHE2//ulOX2e+71FnR/QE1Jj+I/zMvjtjbP5/NxRPP3VswkJETJSrKA4a5Q1msp3yY9LpqZRVtdCu8cwY0Q8P1hsPd/4lBinzwTglTusWk5yTDhTfY57XxesJocfXDmFHyyaQkpMmLW4oEsQsUZZgRVCoS5hb3EdqbHhdHgMz36cx8ZDnZ3C3sl6j63Y5xw7UtnImv3l1sKA6/JYkV3M5+eOJNQVQmKUm8qGNv62Pg+AK2cM47yJVrOONwCOVDTy4sZ8Z7RQelw4JbUt3Lt0G8+tzeN/P8gBYKXdzFZY08w/Nhzpcp2bD1c6o7G8807Gp8YQG+EmLiLUWWn3v97axZPvHwCspdTX5x57smBfvM/nbeZrau3o8nvy1Vt/Rm5ZPUU1TSz+7ep+TS4sqW3m/1bm8MT7B7osAeJtBnzkX7v5zotb+fEbuyira+FASV2PzvWW9g6q7d/RvpI66lv6Dtbn1uZx/q9WdhlpVF7fiitEKK9vobC6iTUHyroEwNs7i/jC0+tYuimfIxWNfYbDevsLyTnjko573adKQ0GdVuGhLsJDezY3ec0cGc+K+y7k6a+cjdsVwtjkaOebuldGilVTOMuuKYy1axN3XTKBcyd09n0sGJ/EdXNGsfaBS1k4IZn545K4ckY6b99zfpcd7zKSo/jJp6dx2/njAOvb8jWzRvCbL87irksmEuoK4aLJVl9GWmwEZ42M54rp1sKDidFhfP28cVw1cxgvf2sh505I5ulVB7l36TYAZ/XbtNhwDpU3EB3mIjrMRXZhLXvsD7bfvLuftg7Dl88ZA0BCVBiHKxp49pM8bpo3mtFJUbhChJvmWSvRx4aHklfRwDKflXK9M9Stn3fTZDd1eD/EEqLcXUYvbTxUyReeXsfPlu/ml//eywsbjhAV5nKayqaPiGNTXiUdHsPLWQU89VEOJbXNfOfFrXzv5e20d3gwxtDhMV0+BBtb251hvd15awjeJrS/rD3EDc+sc0acNbd18OHeElbtL+Ocn7/fZcLgprxKLv3NKm57Pou9xXVO5/ux7LSHJn98oIxPDpYzMiGSpOgwth6posNjWLox3/kd3fb8Jq54YrUz8MDLN5zueXErN/YySACsb/zPr83jaHUTOwqs6/E27c0cYb03Dy/L5qt/3shrWzrft4OlVg3twdd3cuGvVzohWVrXzIKff+A0K244VMnIhMjTsouiP5fOVuqkTBkWy5RhsX0+fumUNHbk1zDD/s/22VkjGJ8azezRCRypbOS8ick89JkZTrOUtyYRG+HmGZ++ES8R4bYLxrNyXyl/+vgQyTHh/OLzn+pyzqIZ6by2pYCj1U2s+sHFtHsM/971DknRYfzQ7mgH+NMtmazeX8Ydf9/CrNEJRLpDOFBSz59vmce/dhQybXgcT686yJvbjmIM3Dx/NHuL6/j83FHO3tyJUW4aWztIig5zOvEBbrtgPKOSIll3sIKXs6xJgr+6/lPMHZPIJznlzoisGzNH88zqzuVBUmPDuWBSCqv3W99SPcbqBDcGXtlc4HSYTx0W6wxvvnxaOo++vYfV+8ucgPnW3zY7s6cX/XY141OiiY8MI7+ykZfvWAjAo2/v4aO9paz50aXO2lxgfWh618byDh32Lk/yxzW5/N+X5vK7Dw7w+48OMj/DWrPr4wPlzvBo78CCbHv0WF81DF/eiZAeA+tzK7lwcipNre3klTdypLKRprYO4iPdFFQ1OpMGu/cbdR9qnF1YS3Vja5d+N7D6hLxLxqw7WMHZY5OoamzFY+Cc8clsL6hhjT3M+vF39xEV5uKqmcOcFYy9thdUc874ZDYeqqS4tpl3s4uZOyaBDYcqucQeZOFvWlNQQ84545N58fYFTgd3WGiIMxFvbHI0L9y24Jih0hdvX0dKTFiPxy6cZA2FnZweQ6grhAi3i9SYcJKju54bFRbK4pnD2fHwIp67dR4/+fR0fnfzHD41Kp4Hr57G5+aMZFRiFB0eQ2iI8J+fmc4bd57HVxeMdZ7Du1T5/7t6Gok+zx8Z5uK6OaMYa/epRLpdfPqsEUxKj3WGJo9MiOSiKVZZvfNOJqXFkDk2ifL6Vg5XNLLmQBmHyhv49sUTMMaqeUDXpTUus5dgf+L9/YA19HdbfjUpMdZkydyyBlbtL+Pd7GI25lU6iwJm5VVSWNPMxznlzuxwsJbiaG6zPngPltXT2u4h63AlEe4Qlu8sYm9xrbMPyEa7RuNbs/n4QDkJ9jVmJEex5UgVuWX1XP3kGjYfriS/spHswhqMMbyzq4iK+hZ2Hq1hUloM9142CbCaXjKSozlU0eCMQls0PZ3mNg8eA9OHx1FS29JleHFpbc9aj+/IL7AC6lt/20xseCjjUqKdvidvgM4YEUek2+WEa1NbB3e+sIXfvLuf/cV1nD02kdsvHA/AvmIrJLwTMDfmVfLQW9lUNrQ676u/aU1BKdvIhEhunj+6y54UXpFhLt686zxnEUKAey+fxPD4iB7ngtVpDnT5UPdaNCOd4tomvnH+uF4766+bM5KYiFCun9v7lube1/z2xROc5inv84xMjGTh+GQev2EWkW4X335hixUKGdaExbv+sQVXiJAQ5ea+yycRGxHKuRNSeO6TQyy2JyWCNdR36rBYdhTUIAIvfnMBr20pID0ugvW5lWTlVZJ1uIq2Dqt56t+7ivjawgynaeiWZzeSEOVm/YOX0dTa4fQnpMSEs7eolv95dx/NbR7uv3wyT7y/n7te2EJjawfD4iIotof3bsqz5qy8t6eEXYU13HfZZK6ZPYK9RbV8+4UtfPlPGyiqaeZnb+9he0ENHR7D+RNT+DinnBsyR7GjoIYLJ6dw/xWTueOiCYSFhvD0qoOUbS4gK6+KELHCz7s0y+fnjmT327U8veogLe0dnD02iSa7c/6eSydSXNvMK5sL2JZfzcVT0liRXcx7u0vYnl9NVLiLv992Dq9kFfD3DYc5UtHodDKnxoYzPjWa7MJabp4/hkc/N5Mfv7GT/1uZ47yPP1o8lT1FtewrscLK2wS19Ug1W49Uc/uF4/msz1Bxf9JQUMoWEiL84vNn9fn47NFd98z+8jlj+zjz2G7IHM0NmaP7fHxYfESXmkN3n501gujw0C7h5e2gnDEiDhHh83NHOd/epw6PY1JaDPdcNonlO4vIKa3nqwvGEh7q4s6LJwLw25t6dv4/fM0MblqyntAQIdQVwo3zrD6Pi6ek0dzWwVn//S6t7R7GpUTzxtajZGYk4jGdnd7VjW3cuGQ9u47WEG3PX7l+7kieWZ3LktW5hIeG8LWFY3k5K5+DZQ0sHJ/MWaPjeWZVrvPh/v1XtvP61qOEuUK4Yno641KiSY4J45xxSewoqGFyegxbjlQT4Q7h8mlpztpa3uY177wW7/wZ78i1FdnFjE+N6TJR85rZI3j07T08tzaPEIF/bS/ihnmjCRFrK1xXiLA9v4aVe0u5ZtYIezVdq9/jf2+ew7ThcXz9/Axe3ZzPd5Zu5evnZQCQGhPOhNQYsgtrmTosFleI8Mi1M1m6yerTmGSXYeqwWJ5fd5h9xXWsz61kdFIk+ZVNhLlCeGDx1H6vXHCqNBSUGmLcrhCunDGsy7HLp6Xz46un8RWfMBmTHMXS2xcw125a++4Vk7n3skl8sKeEc8Ynd3/aHhaMT+Zn181kVGLPzs0It4sLJ6VQ0dDKLQszuO+lbTz4urWY3gu3nYOIcNVv17A9v5rxqdHk2v0Id186ke8umkxDSwftHg+J0WFcMT2d59bmccO8UUwdFsf6gxU8cu0MvvbsRl7fepTLp6XxvzfPdT7Y4yLcvPQtqw9j3cEKbv7jem7IHM39l0+mqmEzl0xN41fv7OW8ick9vl17R64drW7is7NGMCrR6m9KjQ0nLTaCsclRHK5o5Pdfnssdf9/CPzYcISWmc++Si6aksmR1LoufXENru4eF45NJig5zlqkflRjF/7t6Gg+8vtNZOiXFDgXAadYMCw3hL7fO476XtjEvwxpRNHVYHK3tHq787WoAbr9gPOsPVfKdSyf2uQqBP2goKHUGCHWF8E27XdrXgm4f/q4QYVG3QDmWY9WGfnvTHDo8htjwUJ5fl8fWI9XERoQyIdVacPHcicl8tK+Mp79yNofKG8jKqyTWblbzHYH2lQVjaWhp56qZw4lwu3jr7vMBeO4/5vP7j3L40eKpTiD0vL4kHrv+LBbNSCchKszp8J6cHsOs0Qk9vl17awoAXzlnDBFuF8PiIhhnj2h75VsLERFSYsIYlxLNofIGpo/oHNn14FVTuX7uKL749FraOzw8efPsLiPZwKrJPfzPbGsU1bgkEqPDuHRqGh/nlPEpe1FKsIZPb/+vRc79c8YnMTw+gmtmjWB0UhRfOHsUX12Y0efv31/kRGf3BVpmZqbJysoKdDGUUj5qmtp4f3cJwxMiONdepHDX0Rq2F1SfdDObv2Q++h7jUqJ55Y5zAXh9SwHJMeFcNLlrR+7uwlr2FNVy/qQU0uO6fvCvPVjO/uI6bj1vXK+v8Z0Xt/LP7YX88WuZzvDlQBORzcaYnsPvup/nz1AQkcXAk4AL+JMx5pfdHg8H/gqcDVQANxpj8o71nBoKSqlT0dTaQVhoSJchswNtf0kdb249yvcXTTmtTT/H0t9Q8NuQVBFxAU8BVwHTgZtFZHq3074BVBljJgJPAL/yV3mUUgqsTmd/BgJYy8//cPHUQRMIJ8Kf8xTmAznGmFxjTCuwFLi22znXAs/bt18FLpPT1cWulFKqB3+Gwkgg3+d+gX2s13OMMe1ADdBjWISI3C4iWSKSVVZW1v1hpZRSA8SfodDbN/7uHRj9OQdjzBJjTKYxJjM19fTM6lNKqWDkz1AoAHxn6IwCum9f5JwjIqFAPHD8RU2UUkr5hT9DYRMwSUTGiUgYcBOwrNs5y4Bb7NtfAD40Q22MrFJKnUH8NnnNGNMuIncDK7CGpD5rjMkWkUeALGPMMuDPwN9EJAerhnCTv8qjlFLq+Pw6o9kYsxxY3u3YQz63m4Ev+rMMSiml+k+XzlZKKeUYcstciEgZcPgkfzwF6Hv39aFFr2Vw0msZnPRaYKwx5rjDN4dcKJwKEcnqzzTvoUCvZXDSaxmc9Fr6T5uPlFJKOTQUlFJKOYItFJYEugADSK9lcNJrGZz0WvopqPoUlFJKHVuw1RSUUkodQ9CEgogsFpF9IpIjIg8EujwnSkTyRGSniGwTkSz7WJKIvCciB+y/EwNdzt6IyLMiUioiu3yO9Vp2sfzOfp92iMjcwJW8pz6u5WEROWq/N9tE5Gqfxx60r2WfiFwZmFL3JCKjRWSliOwRkWwRudc+PuTel2Ncy1B8XyJEZKOIbLev5b/t4+NEZIP9vrxkLx2EiITb93PsxzNOuRDGmDP+D9YyGweB8UAYsB2YHuhyneA15AEp3Y49Bjxg334A+FWgy9lH2S8E5gK7jld24Grg31gr6C4ANgS6/P24loeB7/dy7nT731o4MM7+N+gK9DXYZRsOzLVvxwL77fIOufflGNcyFN8XAWLs225gg/37fhm4yT7+NPBt+/adwNP27ZuAl061DMFSU+jPhj9Dke8mRc8DnwtgWfpkjFlNz9Vv+yr7tcBfjWU9kCAiw09PSY+vj2vpy7XAUmNMizHmEJCD9W8x4IwxRcaYLfbtOmAP1v4mQ+59Oca19GUwvy/GGFNv33XbfwxwKdZGZNDzfRnQjcqCJRT6s+HPYGeAd0Vks4jcbh9LN8YUgfUfA0gLWOlOXF9lH6rv1d12s8qzPs14Q+Ja7CaHOVjfSof0+9LtWmAIvi8i4hKRbUAp8B5WTabaWBuRQdfy9mujshMRLKHQr818BrnzjDFzsfa8vktELgx0gfxkKL5XfwAmALOBIuA39vFBfy0iEgO8BtxnjKk91qm9HBvs1zIk3xdjTIcxZjbWHjTzgWm9nWb/PeDXEiyh0J8NfwY1Y0yh/Xcp8AbWP5YSbxXe/rs0cCU8YX2Vfci9V8aYEvs/sgf4I51NEYP6WkTEjfUh+oIx5nX78JB8X3q7lqH6vngZY6qBj7D6FBLE2ogMupZ3wDcqC5ZQ6M+GP4OWiESLSKz3NrAI2EXXTYpuAd4KTAlPSl9lXwZ8zR7tsgCo8TZnDFbd2tavw3pvwLqWm+wRIuOAScDG012+3tjtzn8G9hhjHvd5aMi9L31dyxB9X1JFJMG+HQlcjtVHshJrIzLo+b4M7EZlge5tP11/sEZP7Mdqn/txoMtzgmUfjzVaYjuQ7S0/VtvhB8AB+++kQJe1j/K/iFV9b8P6ZvONvsqOVR1+yn6fdgKZgS5/P67lb3ZZd9j/SYf7nP9j+1r2AVcFuvw+5Tofq5lhB7DN/nP1UHxfjnEtQ/F9OQvYapd5F/CQfXw8VnDlAK8A4fbxCPt+jv34+FMtg85oVkop5QiW5iOllFL9oKGglFLKoaGglFLKoaGglFLKoaGglFLKoaGggpaIrLX/zhCRLw3wc/+/3l5LqcFOh6SqoCciF2OtpvmZE/gZlzGm4xiP1xtjYgaifEqdTlpTUEFLRLyrUf4SuMBec/9+e0GyX4vIJnsxtW/Z519sr9v/D6xJUYjIm/YihdnehQpF5JdApP18L/i+lj0j+Nciskus/TFu9Hnuj0TkVRHZKyIvnOpql0qdjNDjn6LUGe8BfGoK9od7jTFmnoiEA5+IyLv2ufOBmcZachng68aYSntJgk0i8pox5gERudtYi5p193msBdpmASn2z6y2H5sDzMBa1+YT4Dzg44G/XKX6pjUFpXpahLXOzzasJZiTsdbHAdjoEwgA94jIdmA91sJkkzi284EXjbVQWwmwCpjn89wFxlrAbRuQMSBXo9QJ0JqCUj0J8B1jzIouB62+h4Zu9y8HFhpjGkXkI6y1aI733H1p8bndgf7/VAGgNQWloA5rG0evFcC37eWYEZHJ9uq03cUDVXYgTMVa4tirzfvz3awGbrT7LVKxtvccFCt0KgX6TUQpsFakbLebgZ4DnsRqutlid/aW0ftWp+8Ad4jIDqzVNtf7PLYE2CEiW4wxX/Y5/gawEGvFWwP80BhTbIeKUgGnQ1KVUko5tPlIKaWUQ0NBKaWUQ0NBKaWUQ0NBKaWUQ0NBKaWUQ0NBKaWUQ0NBKaWUQ0NBKaWU4/8DFMgBBPyujDIAAAAASUVORK5CYII=\n",
                        "text/plain": "<Figure size 432x288 with 1 Axes>"
                    },
                    "metadata": {
                        "needs_background": "light"
                    },
                    "output_type": "display_data"
                }
            ],
            "source": "plt.plot(loss_list)\nplt.xlabel(\"iteration\")\nplt.ylabel(\"loss\")\nplt.show()"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "<h2 id=\"Question_3\">Question 3:Find the misclassified samples</h2> "
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "<b>Identify the first four misclassified samples using the validation data:</b>"
        },
        {
            "cell_type": "code",
            "execution_count": 27,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "sample 1, predicted value: tensor([1]), actual value: tensor([0])\nsample 166, predicted value: tensor([0]), actual value: tensor([1])\nsample 306, predicted value: tensor([0]), actual value: tensor([1])\nsample 456, predicted value: tensor([0]), actual value: tensor([1])\n"
                }
            ],
            "source": "j = 0\nsingle_loader = torch.utils.data.DataLoader(\n    dataset=validation_dataset,\n    batch_size=1)\n\nfor i, (x, y) in enumerate(single_loader):\n    model.eval() # evaluate model\n    z = model(x) # make a prediction\n    _, yhat = torch.max(z.data, 1) # find max\n    \n    if yhat == y:\n        continue\n    \n    j += 1\n    print(f\"sample {i}, predicted value: {yhat}, actual value: {y}\")\n    \n    if j >= 4:\n        break"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "<a href=\"https://dataplatform.cloud.ibm.com/docs/content/wsj/analyze-data/share-notebooks.html\"> CLICK HERE </a> Click here to see how to share your notebook."
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "<h2>About the Authors:</h2> \n\n<a href=\"https://www.linkedin.com/in/joseph-s-50398b136/\">Joseph Santarcangelo</a> has a PhD in Electrical Engineering, his research focused on using machine learning, signal processing, and computer vision to determine how videos impact human cognition. Joseph has been working for IBM since he completed his PhD."
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "Copyright &copy; 2018 <a href=\"cognitiveclass.ai?utm_source=bducopyrightlink&utm_medium=dswb&utm_campaign=bdu\">cognitiveclass.ai</a>. This notebook and its source code are released under the terms of the <a href=\"https://bigdatauniversity.com/mit-license/\">MIT License</a>."
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": ""
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3.6",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.6.9"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}